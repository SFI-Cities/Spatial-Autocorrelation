{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from osgeo import ogr\n",
    "import pandas as pd\n",
    "\n",
    "from us import states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_csv = pd.read_csv('/Users/joe/Dropbox/SFI_Census_Stuff/2010Census/2010.csv', dtype={'ID': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download & extract all zip files\n",
    "2. ??\n",
    "3. make ID column\n",
    "4. combine census data & shapefiles\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_shape(fips, extract_path):\n",
    "    \"\"\"\n",
    "        gz_2010_ss_150_00_500k.zip, where ss is the 2 digit state FIPS code.\n",
    "    \"\"\"\n",
    "    shapefile_url = 'http://www2.census.gov/geo/tiger/GENZ2010/'\n",
    "    shapefile_url += 'gz_2010_{}_150_00_500k.zip'.format(fips)\n",
    "    \n",
    "    r = requests.get(shapefile_url)\n",
    "    zfile = zipfile.ZipFile(BytesIO(r.content))\n",
    "    zfile.extractall(path=extract_path)\n",
    "    return zfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_id_field(file_path):\n",
    "    source = ogr.Open(file_path, 1)\n",
    "    layer = source.GetLayer()\n",
    "    layer_defn = layer.GetLayerDefn()\n",
    "    field_names = [layer_defn.GetFieldDefn(i).GetName() for i in range(layer_defn.GetFieldCount())]\n",
    "\n",
    "    # Add a new field\n",
    "    new_field = ogr.FieldDefn('ID', ogr.OFTString)\n",
    "    layer.CreateField(new_field)\n",
    "\n",
    "    feature = layer.GetNextFeature()\n",
    "    while feature:\n",
    "        feature.SetField(\"ID\", feature.GetField(\"GEO_ID\").split('US')[1] )\n",
    "        layer.SetFeature(feature)\n",
    "        feature = layer.GetNextFeature()\n",
    "\n",
    "    # Close the Shapefile\n",
    "    source.Destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_files(directory, outputMergefn, fileStartsWith='gz_'):\n",
    "    fileEndsWith = '.shp'\n",
    "    driverName = 'ESRI Shapefile'\n",
    "    geometryType = ogr.wkbPolygon\n",
    "\n",
    "    out_driver = ogr.GetDriverByName( driverName )\n",
    "    if os.path.exists(outputMergefn):\n",
    "        out_driver.DeleteDataSource(outputMergefn)\n",
    "    out_ds = out_driver.CreateDataSource(outputMergefn)\n",
    "    out_layer = out_ds.CreateLayer(outputMergefn, geom_type=geometryType)\n",
    "\n",
    "    fileList = os.listdir(directory)\n",
    "\n",
    "    create_fields = True\n",
    "    for file in fileList:\n",
    "        if file.startswith(fileStartsWith) and file.endswith(fileEndsWith):\n",
    "            print (file)\n",
    "            ds = ogr.Open(directory+file)\n",
    "            lyr = ds.GetLayer()\n",
    "            lyr_def = lyr.GetLayerDefn()\n",
    "            \n",
    "            if create_fields:\n",
    "                # Only create new fields once\n",
    "                # Create OLD Fields\n",
    "                for i in range(lyr_def.GetFieldCount()):\n",
    "                    fieldName = lyr_def.GetFieldDefn(i).GetName()              \n",
    "                    fieldTypeCode =lyr_def.GetFieldDefn(i).GetType()\n",
    "\n",
    "                    new_field = ogr.FieldDefn(fieldName, fieldTypeCode)\n",
    "                    out_layer.CreateField(new_field)\n",
    "                    \n",
    "                # Create fields from CSV\n",
    "                dtypes = {'float64':ogr.OFTReal, 'int64':ogr.OFTInteger, 'object':ogr.OFTString}\n",
    "                for col in census_csv.columns:\n",
    "                    #new_field = ogr.FieldDefn(col, dtypes[str(census_csv[col].dtype)])\n",
    "                    new_field = ogr.FieldDefn(col, ogr.OFTString)\n",
    "                    out_layer.CreateField(new_field)\n",
    "\n",
    "                # Create ID field\n",
    "                new_field = ogr.FieldDefn('ID', ogr.OFTString)\n",
    "                out_layer.CreateField(new_field)\n",
    "\n",
    "                # Create Density field\n",
    "                new_field = ogr.FieldDefn('Density', ogr.OFTReal)\n",
    "                out_layer.CreateField(new_field)\n",
    "                \n",
    "                create_fields = False\n",
    "\n",
    "            for feat in lyr:\n",
    "                feat_id = feat.GetField(\"GEO_ID\").split('US')[1]\n",
    "                pd_row = census_csv[census_csv.ID == feat_id]\n",
    "                if len(pd_row) > 0:\n",
    "                    out_feat = ogr.Feature(out_layer.GetLayerDefn())\n",
    "                    out_feat.SetGeometry(feat.GetGeometryRef().Clone())\n",
    "                    # Only get features in our dataset (i.e. in cities)\n",
    "\n",
    "                    # Add old fields to new feature\n",
    "                    old_fields = [lyr_def.GetFieldDefn(i).GetName() for i in range(lyr_def.GetFieldCount())]\n",
    "                    for field in old_fields:\n",
    "                        out_feat.SetField(field, feat.GetField(field))\n",
    "                    # Add new ID field\n",
    "                    out_feat.SetField(\"ID\",  feat_id)\n",
    "                    # Add new Density field\n",
    "                    if feat.GetField(\"CENSUSAREA\") > 0:\n",
    "                        density = float(pd_row['TOTPOP'])/feat.GetField(\"CENSUSAREA\")\n",
    "                        out_feat.SetField(\"Density\",  density)\n",
    "                    else:\n",
    "                        out_feat.SetField(\"Density\",  0)\n",
    "                        \n",
    "                    \n",
    "                    for col in pd_row.columns:\n",
    "                        out_feat.SetField(str(col), str(pd_row.iloc[0].to_dict()[col]))\n",
    "                \n",
    "                    out_layer.CreateFeature(out_feat)\n",
    "                    out_layer.SyncToDisk()\n",
    "            ds.Destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gz_2010_01_150_00_500k.shp\n",
      "gz_2010_02_150_00_500k.shp\n",
      "gz_2010_04_150_00_500k.shp\n",
      "gz_2010_05_150_00_500k.shp\n",
      "gz_2010_06_150_00_500k.shp\n",
      "gz_2010_08_150_00_500k.shp\n",
      "gz_2010_09_150_00_500k.shp\n",
      "gz_2010_10_150_00_500k.shp\n",
      "gz_2010_11_150_00_500k.shp\n",
      "gz_2010_12_150_00_500k.shp\n",
      "gz_2010_13_150_00_500k.shp\n",
      "gz_2010_15_150_00_500k.shp\n",
      "gz_2010_16_150_00_500k.shp\n",
      "gz_2010_17_150_00_500k.shp\n",
      "gz_2010_18_150_00_500k.shp\n",
      "gz_2010_19_150_00_500k.shp\n",
      "gz_2010_20_150_00_500k.shp\n",
      "gz_2010_21_150_00_500k.shp\n",
      "gz_2010_22_150_00_500k.shp\n",
      "gz_2010_23_150_00_500k.shp\n",
      "gz_2010_24_150_00_500k.shp\n",
      "gz_2010_25_150_00_500k.shp\n",
      "gz_2010_26_150_00_500k.shp\n",
      "gz_2010_27_150_00_500k.shp\n",
      "gz_2010_28_150_00_500k.shp\n",
      "gz_2010_29_150_00_500k.shp\n",
      "gz_2010_30_150_00_500k.shp\n",
      "gz_2010_31_150_00_500k.shp\n",
      "gz_2010_32_150_00_500k.shp\n",
      "gz_2010_33_150_00_500k.shp\n",
      "gz_2010_34_150_00_500k.shp\n",
      "gz_2010_35_150_00_500k.shp\n",
      "gz_2010_36_150_00_500k.shp\n",
      "gz_2010_37_150_00_500k.shp\n",
      "gz_2010_38_150_00_500k.shp\n",
      "gz_2010_39_150_00_500k.shp\n",
      "gz_2010_40_150_00_500k.shp\n",
      "gz_2010_41_150_00_500k.shp\n",
      "gz_2010_42_150_00_500k.shp\n",
      "gz_2010_44_150_00_500k.shp\n",
      "gz_2010_45_150_00_500k.shp\n",
      "gz_2010_46_150_00_500k.shp\n",
      "gz_2010_47_150_00_500k.shp\n",
      "gz_2010_48_150_00_500k.shp\n",
      "gz_2010_49_150_00_500k.shp\n",
      "gz_2010_50_150_00_500k.shp\n",
      "gz_2010_51_150_00_500k.shp\n",
      "gz_2010_53_150_00_500k.shp\n",
      "gz_2010_54_150_00_500k.shp\n",
      "gz_2010_55_150_00_500k.shp\n",
      "gz_2010_56_150_00_500k.shp\n"
     ]
    }
   ],
   "source": [
    "def download_merge():\n",
    "    temp_path = 'tmp'\n",
    "    for state in states.STATES:\n",
    "        continue\n",
    "        #download_shape(state.fips, temp_path)\n",
    "    merge_files(temp_path + '/', 'us_bkgps_2010.shp')\n",
    "download_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
